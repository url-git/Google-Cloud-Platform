{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO1J522lWJLoyDRZWxgeTy3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[012 Serverless Data Processing with Dataflow - Writing an ETL Pipeline using Apache Beam and Dataflow (Python)](https://www.cloudskillsboost.google/focuses/64780?catalog_rank=%7B%22rank%22%3A3%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=41740668)"],"metadata":{"id":"K7tdBFaccMMv"}},{"cell_type":"markdown","source":["#Setup"],"metadata":{"id":"KKrpvqMdvCoN"}},{"cell_type":"markdown","source":["##SA"],"metadata":{"id":"aD98wLbSevy_"}},{"cell_type":"code","source":["{project-number}-compute@developer.gserviceaccount.comjest # editor"],"metadata":{"id":"sfE9P5hCeVoe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Konfiguracja środowiska programistycznego opartego na notebooku Jupyter"],"metadata":{"id":"HzlTolIte80y"}},{"cell_type":"code","source":["Vertex AI > Workbench"],"metadata":{"id":"LiorKk04e0ZK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Repo notebook Jupyter"],"metadata":{"id":"kkkxpJMufE_I"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Vi4wbNgaj0o"},"outputs":[],"source":["git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n","cd /home/jupyter/training-data-analyst/quests/dataflow_python/"]},{"cell_type":"markdown","source":["#Część 1 laboratorium. Pisanie potoku ETL od podstaw"],"metadata":{"id":"EgRCdAlBITEA"}},{"cell_type":"code","source":["cd 1_Basic_ETL/lab\n","export BASE_DIR=$(pwd)"],"metadata":{"id":"UKsEAdGHsNXT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Skonfiguruj środowisko wirtualne i zależności"],"metadata":{"id":"GJMKmAF5s1CB"}},{"cell_type":"code","source":["# Pobiera najnowsze informacje o dostępnych pakietach.\n","# python3-venv umożliwia tworzenie wirtualnych środowisk Pythona (venv).\n","# Opcja -y automatycznie akceptuje instalację.\n","\n","sudo apt-get update && sudo apt-get install -y python3-venv"],"metadata":{"id":"WMPxlnGes5Jq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##venv"],"metadata":{"id":"w2RW-rc4ubVo"}},{"cell_type":"code","source":["python3 -m venv df-env\n","source df-env/bin/activate"],"metadata":{"id":"SexgEKvKt6u6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Instalacja pakietów"],"metadata":{"id":"fKZFrpKHua1c"}},{"cell_type":"code","source":["python3 -m pip install -q --upgrade pip setuptools wheel\n","python3 -m pip install apache-beam[gcp]"],"metadata":{"id":"x78Ej_UAuZQe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##API Dataflow"],"metadata":{"id":"bP-9WSAVvAUa"}},{"cell_type":"code","source":["gcloud services enable dataflow.googleapis.com"],"metadata":{"id":"tHF_1j6Ju_sY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Zadanie 1. Generowanie danych syntetycznych"],"metadata":{"id":"486AealFvedC"}},{"cell_type":"code","source":["cd $BASE_DIR/../..\n","\n","source create_batch_sinks.sh\n","\n","bash generate_batch_events.sh\n","\n","head events.json"],"metadata":{"id":"A9mTtCyLvfCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["{\"user_id\": \"-6434255326544341291\", \"ip\": \"192.175.49.116\", \"timestamp\": \"2019-06-19T16:06:45.118306Z\", \"http_request\": \"\\\"GET eucharya.html HTTP/1.0\\\"\", \"lat\": 37.751, \"lng\": -97.822, \"http_response\": 200, \"user_agent\": \"Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.01; Trident/5.1)\", \"num_bytes\": 182}"],"metadata":{"id":"Pgom9vHHvmgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","# Dane wejściowe w formacie JSON\n","data = {\n","    \"user_id\": \"-6434255326544341291\",\n","    \"ip\": \"192.175.49.116\",\n","    \"timestamp\": \"2019-06-19T16:06:45.118306Z\",\n","    \"http_request\": \"\\\"GET eucharya.html HTTP/1.0\\\"\",\n","    \"lat\": 37.751,\n","    \"lng\": -97.822,\n","    \"http_response\": 200,\n","    \"user_agent\": \"Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.01; Trident/5.1)\",\n","    \"num_bytes\": 182\n","}\n","\n","# Drukowanie sformatowanych danych JSON\n","print(json.dumps(data, indent=4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mna3fD-3v0VP","executionInfo":{"status":"ok","timestamp":1738853543197,"user_tz":-60,"elapsed":18,"user":{"displayName":"Piotr Maćkówka","userId":"03621941332162417804"}},"outputId":"ed682e4f-09a7-4893-997a-5464a7735176"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"user_id\": \"-6434255326544341291\",\n","    \"ip\": \"192.175.49.116\",\n","    \"timestamp\": \"2019-06-19T16:06:45.118306Z\",\n","    \"http_request\": \"\\\"GET eucharya.html HTTP/1.0\\\"\",\n","    \"lat\": 37.751,\n","    \"lng\": -97.822,\n","    \"http_response\": 200,\n","    \"user_agent\": \"Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.01; Trident/5.1)\",\n","    \"num_bytes\": 182\n","}\n"]}]},{"cell_type":"markdown","source":["#Zadanie 2. Odczytaj dane ze źródła"],"metadata":{"id":"L6KAGuFvH1NF"}},{"cell_type":"code","source":["import argparse\n","import time\n","import logging\n","import json\n","import apache_beam as beam\n","from apache_beam.options.pipeline_options import GoogleCloudOptions\n","from apache_beam.options.pipeline_options import PipelineOptions\n","from apache_beam.options.pipeline_options import StandardOptions\n","from apache_beam.runners import DataflowRunner, DirectRunner\n","\n","# ### main\n","\n","def run():\n","    # Command line arguments\n","    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n","    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n","    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n","    parser.add_argument('--stagingLocation', required=True, help='Specify Cloud Storage bucket for staging')\n","    parser.add_argument('--tempLocation', required=True, help='Specify Cloud Storage bucket for temp')\n","    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n","\n","    opts = parser.parse_args()\n","\n","    # Setting up the Beam pipeline options\n","    options = PipelineOptions()\n","    options.view_as(GoogleCloudOptions).project = opts.project\n","    options.view_as(GoogleCloudOptions).region = opts.region\n","    options.view_as(GoogleCloudOptions).staging_location = opts.stagingLocation\n","    options.view_as(GoogleCloudOptions).temp_location = opts.tempLocation\n","    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n","    options.view_as(StandardOptions).runner = opts.runner\n","\n","    # Static input and output\n","    input = 'gs://{0}/events.json'.format(opts.project)\n","    output = '{0}:logs.logs'.format(opts.project)\n","\n","    # Table schema for BigQuery\n","    table_schema = {\n","        \"fields\": [\n","            {\n","                \"name\": \"ip\",\n","                \"type\": \"STRING\"\n","            },\n","            {\n","                \"name\": \"user_id\",\n","                \"type\": \"STRING\"\n","            },\n","            {\n","                \"name\": \"lat\",\n","                \"type\": \"FLOAT\"\n","            },\n","            {\n","                \"name\": \"lng\",\n","                \"type\": \"FLOAT\"\n","            },\n","            {\n","                \"name\": \"timestamp\",\n","                \"type\": \"STRING\"\n","            },\n","            {\n","                \"name\": \"http_request\",\n","                \"type\": \"STRING\"\n","            },\n","            {\n","                \"name\": \"http_response\",\n","                \"type\": \"INTEGER\"\n","            },\n","            {\n","                \"name\": \"num_bytes\",\n","                \"type\": \"INTEGER\"\n","            },\n","            {\n","                \"name\": \"user_agent\",\n","                \"type\": \"STRING\"\n","            }\n","        ]\n","    }\n","\n","    # Create the pipeline\n","    p = beam.Pipeline(options=options)\n","\n","    '''\n","\n","    Steps:\n","    1) Read something\n","    2) Transform something\n","    3) Write something\n","\n","    '''\n","\n","    (p\n","        | 'ReadFromGCS' >> beam.io.ReadFromText(input)\n","        | 'ParseJson' >> beam.Map(lambda line: json.loads(line))\n","        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n","            output,\n","            schema=table_schema,\n","            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n","            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n","            )\n","    )\n","\n","    logging.getLogger().setLevel(logging.INFO)\n","    logging.info(\"Building pipeline ...\")\n","\n","    p.run()\n","\n","if __name__ == '__main__':\n","  run()"],"metadata":{"id":"604oZvu1HvGI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import argparse  # Moduł do parsowania argumentów wiersza poleceń\n","import time  # Moduł do operacji na czasie (np. generowanie unikalnej nazwy zadania)\n","import logging  # Moduł do logowania informacji\n","import json  # Moduł do parsowania danych JSON\n","import apache_beam as beam  # Import głównej biblioteki Apache Beam\n","from apache_beam.options.pipeline_options import GoogleCloudOptions  # Opcje dla GCP\n","from apache_beam.options.pipeline_options import PipelineOptions  # Opcje dla potoku\n","from apache_beam.options.pipeline_options import StandardOptions  # Standardowe opcje dla Apache Beam\n","from apache_beam.runners import DataflowRunner, DirectRunner  # Możliwe wykonawce Apache Beam\n","\n","# Definicja głównej funkcji uruchamiającej potok\n","\n","def run():\n","    # Parsowanie argumentów wiersza poleceń\n","    parser = argparse.ArgumentParser(description='Load from JSON into BigQuery')\n","    parser.add_argument('--project', required=True, help='Specify Google Cloud project')  # Identyfikator projektu GCP\n","    parser.add_argument('--region', required=True, help='Specify Google Cloud region')  # Region, w którym działa Dataflow\n","    parser.add_argument('--stagingLocation', required=True, help='Specify Cloud Storage bucket for staging')  # Miejsce przechowywania plików pośrednich\n","    parser.add_argument('--tempLocation', required=True, help='Specify Cloud Storage bucket for temp')  # Lokalizacja dla plików tymczasowych\n","    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')  # Wybór wykonawcy np. Dataflow lub DirectRunner\n","\n","    opts = parser.parse_args()  # Pobranie wartości argumentów\n","\n","    # Ustawienie opcji potoku Apache Beam\n","    options = PipelineOptions()\n","    options.view_as(GoogleCloudOptions).project = opts.project  # Przypisanie projektu GCP\n","    options.view_as(GoogleCloudOptions).region = opts.region  # Ustawienie regionu GCP\n","    options.view_as(GoogleCloudOptions).staging_location = opts.stagingLocation  # Miejsce przechowywania plików pośrednich\n","    options.view_as(GoogleCloudOptions).temp_location = opts.tempLocation  # Miejsce przechowywania plików tymczasowych\n","    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-', time.time_ns())  # Unikalna nazwa zadania\n","    options.view_as(StandardOptions).runner = opts.runner  # Ustawienie wykonawcy (np. Dataflow)\n","\n","    # Ścieżka do wejściowego pliku JSON w Google Cloud Storage\n","    input = 'gs://{0}/events.json'.format(opts.project)\n","    # Nazwa tabeli docelowej w BigQuery\n","    output = '{0}:logs.logs'.format(opts.project)\n","\n","    # Definicja schematu tabeli BigQuery\n","    table_schema = {\n","        \"fields\": [\n","            {\"name\": \"ip\", \"type\": \"STRING\"},\n","            {\"name\": \"user_id\", \"type\": \"STRING\"},\n","            {\"name\": \"lat\", \"type\": \"FLOAT\"},\n","            {\"name\": \"lng\", \"type\": \"FLOAT\"},\n","            {\"name\": \"timestamp\", \"type\": \"STRING\"},\n","            {\"name\": \"http_request\", \"type\": \"STRING\"},\n","            {\"name\": \"http_response\", \"type\": \"INTEGER\"},\n","            {\"name\": \"num_bytes\", \"type\": \"INTEGER\"},\n","            {\"name\": \"user_agent\", \"type\": \"STRING\"}\n","        ]\n","    }\n","\n","    # Tworzenie potoku Apache Beam\n","    p = beam.Pipeline(options=options)\n","\n","    # Definicja kroków potoku\n","    (\n","        p\n","        | 'ReadFromGCS' >> beam.io.ReadFromText(input)  # Odczyt danych JSON z pliku w Google Cloud Storage\n","        | 'ParseJson' >> beam.Map(lambda line: json.loads(line))  # Konwersja każdej linii JSON na obiekt Python (słownik)\n","        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n","            output,  # Nazwa tabeli docelowej\n","            schema=table_schema,  # Schemat tabeli BigQuery\n","            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,  # Tworzenie tabeli, jeśli nie istnieje\n","            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE  # Nadpisanie istniejących danych\n","        )\n","    )\n","\n","    # Ustawienie poziomu logowania i uruchomienie potoku\n","    logging.getLogger().setLevel(logging.INFO)\n","    logging.info(\"Building pipeline ...\")\n","    p.run()\n","\n","# Uruchomienie potoku, jeśli skrypt jest wykonywany bezpośrednio\n","if __name__ == '__main__':\n","    run()\n"],"metadata":{"id":"FM-cVDhEKJKa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Zadanie 3. Uruchom swój potok, aby sprawdzić, czy działa"],"metadata":{"id":"oyMF8IvJjQs3"}},{"cell_type":"code","source":["# Przechodzi do katalogu bazowego, który powinien być wcześniej ustawiony (export BASE_DIR=/ścieżka/do/projektu).\n","cd $BASE_DIR\n","\n","# Pobiera identyfikator bieżącego projektu GCP i zapisuje go w zmiennej PROJECT_ID.\n","export PROJECT_ID=$(gcloud config get-value project)\n","\n","python3 my_pipeline.py \\\n","  --project=${PROJECT_ID} \\ # określa projekt GCP.\n","  --region=Region \\ # wymaga uzupełnienia poprawnego regionu, np. us-central1.\n","  --stagingLocation=gs://$PROJECT_ID/staging/ \\ # miejsce w GCS na pliki stagingowe (np. zależności).\n","  --tempLocation=gs://$PROJECT_ID/temp/ \\ # miejsce w GCS na pliki tymczasowe.\n","  --runner=DirectRunner # wykonuje pipeline lokalnie, bez wysyłania do Dataflow."],"metadata":{"id":"ytW1qpl1jRgT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Zadanie 4. Dodaj transformację"],"metadata":{"id":"I27WM1YsrAFA"}},{"cell_type":"code","source":["[Output_PCollection] = ([Input_PCollection] | [First Transform]\n","                                            | [Second Transform]\n","                                            | [Third Transform])"],"metadata":{"id":"yxSYtUvcrA0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p | beam.Map(lambda x : something(x))"],"metadata":{"id":"gyFJol5w0vuA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def something(x):\n","  y = # Do something!\n","  return y\n","\n","p | beam.Map(something)"],"metadata":{"id":"CcmWcCVQ0x17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyDoFn(beam.DoFn):\n","  def process(self, element):\n","    output = #Do Something!\n","    yield output\n","\n","p | beam.ParDo(MyDoFn())"],"metadata":{"id":"mpGOnnFG00Qn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Zadanie 5. Napisz do ujścia"],"metadata":{"id":"9tZ4aCAM1Jff"}},{"cell_type":"code","source":["# Examine dataset\n","bq ls\n","\n","# No tables yet\n","bq ls logs"],"metadata":{"id":"5SfZ5SHE1J_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# piotr_mackowka@cloudshell:~ (avon-prod-analytics)$ bq ls analytics\n","#               tableId                Type    Labels     Time Partitioning      Clustered Fields\n","#  ---------------------------------- ------- -------- ------------------------ ------------------\n","#   002_purchase                       TABLE            DAY (field: true_date)\n","#   002_select_item                    TABLE            DAY (field: true_date)\n","#   002_union_all                      TABLE            DAY (field: true_date)\n","#   002_view_cart                      TABLE            DAY (field: true_date)\n","#   002_view_item                      TABLE            DAY (field: true_date)\n","#   002_view_item_list                 TABLE            DAY (field: true_date)\n","#   003_basic_kpi                      TABLE            DAY (field: true_date)\n","#   003_basic_kpi_event                TABLE            DAY (field: true_date)\n","#   003_basic_kpi_transactions         TABLE            DAY (field: true_date)\n","#   003_event_counter                  TABLE            DAY (field: true_date)\n","#   004_generate_lead                  TABLE            DAY (field: true_date)\n","#   004_generate_lead_lag              TABLE\n","#   004_generate_lead_sessions         TABLE            DAY (field: true_date)\n","#   005_benchmark                      TABLE            DAY (field: true_date)\n","#   005_click_avon_nagradza            TABLE            DAY (field: true_date)\n","#   005_hamburger_menu_avon_nagradza   TABLE            DAY (field: true_date)\n","#   006_product_performance_campaign   TABLE            DAY (field: true_date)"],"metadata":{"id":"6gwyVkeH2VGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["table_schema = 'name:STRING,id:INTEGER,balance:FLOAT'"],"metadata":{"id":"H5AADy-DBB2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["table_schema = {\n","        \"fields\": [\n","            {\n","                \"name\": \"name\",\n","                \"type\": \"STRING\"\n","            },\n","            {\n","                \"name\": \"id\",\n","                \"type\": \"INTEGER\",\n","                \"mode\": \"REQUIRED\"\n","            },\n","            {\n","                \"name\": \"balance\",\n","                \"type\": \"FLOAT\",\n","                \"mode\": \"REQUIRED\"\n","            }\n","        ]\n","    }\n"],"metadata":{"id":"68Z7jvwYBQM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n","            'project:dataset.table',\n","            schema=table_schema,\n","            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n","            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n","            )"],"metadata":{"id":"XnopA8llBk2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n","            'project:dataset.table',  # 🔹 Pełna nazwa tabeli BigQuery w formacie `projekt:dataset.tabela`\n","            schema=table_schema,  # 🔹 Definicja schematu tabeli (lista pól i ich typów)\n","            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,  # 🔹 Tworzy tabelę, jeśli nie istnieje\n","\n","'''Uwaga:WRITE_TRUNCATE usunie i utworzy ponownie Twoją tabelę za każdym razem.\n","Jest to pomocne na wczesnym etapie iteracji potoku, szczególnie gdy iterujesz swój schemat,\n","ale może łatwo spowodować niezamierzone problemy w produkcji. WRITE_APPEND lub WRITE_EMPTY są bezpieczniejsze.'''\n","            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE  # 🔹 Nadpisuje tabelę przy każdym uruchomieniu potoku\n",")"],"metadata":{"id":"-jvKyJ6NBx_p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Zadanie 6. Uruchom swój potok"],"metadata":{"id":"Uzohok_1C5P-"}},{"cell_type":"code","source":["# Set up environment variables\n","cd $BASE_DIR\n","export PROJECT_ID=$(gcloud config get-value project)\n","\n","# Run the pipelines\n","python3 my_pipeline.py \\\n","  --project=${PROJECT_ID} \\\n","  --region=Region \\\n","  --stagingLocation=gs://$PROJECT_ID/staging/ \\\n","  --tempLocation=gs://$PROJECT_ID/temp/ \\\n","  --runner=DataflowRunner"],"metadata":{"id":"P44ejjUZC55w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Część 2 laboratorium. Parametryzacja podstawowego ETL"],"metadata":{"id":"U1y3JKx1ILJG"}},{"cell_type":"markdown","source":["#Zadanie 1. Utwórz plik schematu JSON"],"metadata":{"id":"Bg6H9HYkSkYO"}},{"cell_type":"code","source":["cd $BASE_DIR/../..\n","\n","'''\t•\tWyświetla schemat tabeli logs.logs w BigQuery w formacie JSON, co ułatwia czytanie i analizę struktury danych.\n","\t•\t--schema → Pobiera tylko schemat tabeli, bez zawartości.\n","\t•\t--format=prettyjson → Formatuje wyjście jako czytelny JSON.'''\n","bq show --schema --format=prettyjson logs.logs"],"metadata":{"id":"gjEIzaPVIL9x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bq show --schema --format=prettyjson logs.logs | sed '1s/^/{\"BigQuery Schema\":/' | sed '$s/$/}/' > schema.json\n","\n","cat schema.json\n","\n","export PROJECT_ID=$(gcloud config get-value project)\n","gcloud storage cp schema.json gs://${PROJECT_ID}/"],"metadata":{"id":"a80RH3l8RORy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Zadanie 2. Napisz funkcję zdefiniowaną przez użytkownika w JavaScript"],"metadata":{"id":"Skj94POLSbsp"}},{"cell_type":"code","source":["function transform(line) {\n","  return line;\n","}"],"metadata":{"id":"sL3XnV6WScPv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Zadanie 3. Uruchom szablon przepływu danych\n"],"metadata":{"id":"odUK3zycTXgm"}},{"cell_type":"markdown","source":["#Zadanie 4. Przejrzyj kod szablonu przepływu danych"],"metadata":{"id":"qusQejEeThS5"}}]}